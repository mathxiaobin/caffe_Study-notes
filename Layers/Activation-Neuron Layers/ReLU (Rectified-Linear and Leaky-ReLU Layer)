本文引用自＂http://caffe.berkeleyvision.org/tutorial/layers/relu.html＂，外加自己注释
Sample　　　　　
　　　　layer {
  　　　　　　　name: "relu1"
  　　　　　　　type: "ReLU"
  　　　　　　　bottom: "conv1"
 　　　　　　　 top: "conv1"
　　　　　　　}
Given an input value x, The ReLU layer computes the output as x if x > 0 and negative_slope * x if x <= 0. When the negative slope parameter is not set, it is equivalent to the standard ReLU function of taking max(x, 0). It also supports in-place computation, meaning that the bottom and the top blob could be the same to preserve memory consumption.
Parameters　
    Optional
        negative_slope [default 0]: specifies whether to leak the negative part by multiplying it with the slope value rather than setting it to 0.
常用的激活层，优点就是快（当然是在保证效果的基础上）
